{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Paper on LSTM Neural Networks for stock prices volatility prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://cs230.stanford.edu/projects_fall_2019/reports/26254244.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial for building an LSTM neural network for time-series prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Datetime\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Scikit-Learn\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# GARCH model\n",
    "\n",
    "from arch import arch_model\n",
    "\n",
    "# Keras\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Tensorflow\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the csv file with the financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            StockCode  Open  Close  High   Low      Volume\n",
      "Date                                                      \n",
      "2020-07-01         68  3.52   3.44  3.52  3.37  25483461.0\n",
      "2020-07-02         68  3.40   3.45  3.45  3.38  17180117.0\n",
      "2020-07-03         68  3.45   3.42  3.46  3.38  17171744.0\n",
      "2020-07-06         68  3.42   3.64  3.76  3.42  48155956.0\n",
      "2020-07-07         68  3.71   3.61  3.79  3.53  42723215.0\n",
      "(131424, 6)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('processed_stock_data.csv')\n",
    "# 只保留需要的列\n",
    "df = df[['股票代码_Stkcd', '日期_Date', '开盘价_Oppr', '收盘价_Clpr', '最高价_Hipr', '最低价_Lopr', '成交量_Trdvol']]\n",
    "\n",
    "# 重命名列名\n",
    "df = df.rename(columns={\n",
    "    '股票代码_Stkcd': 'StockCode',\n",
    "    '日期_Date': 'Date', \n",
    "    '开盘价_Oppr': 'Open',\n",
    "    '收盘价_Clpr': 'Close',\n",
    "    '最高价_Hipr': 'High',\n",
    "    '最低价_Lopr': 'Low',\n",
    "    '成交量_Trdvol': 'Volume'\n",
    "})\n",
    "df.set_index('Date', inplace=True)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "print (df.head())\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see here, we have 254 columns, corresponding to the 254 business days for which we have financial data and 10 columns, which are the 10 financial indicators we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.rename(columns={\n",
    "    \n",
    "#     df.columns[0]: 'Date',\n",
    "#     df.columns[1]:'Open',\n",
    "#     df.columns[2]: 'Close',\n",
    "#     df.columns[3]:'High',\n",
    "#     df.columns[4]:'Low',\n",
    "#     df.columns[5]: 'Volume',\n",
    "#     df.columns[6]: 'RSI14',\n",
    "#     df.columns[7]:'SMA14',\n",
    "#     df.columns[8]: 'EMA14',\n",
    "#     df.columns[9]:'MACD_sl',\n",
    "#     df.columns[10]:'MACD_h'\n",
    "\n",
    "# })\n",
    "\n",
    "# print (df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Date column into a Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Date'] =  pd.to_datetime(df['Date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithmic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            StockCode  Open  Close  High   Low      Volume  Log_Returns\n",
      "Date                                                                   \n",
      "2020-07-01         68  3.52   3.44  3.52  3.37  25483461.0          NaN\n",
      "2020-07-02         68  3.40   3.45  3.45  3.38  17180117.0     0.002903\n",
      "2020-07-03         68  3.45   3.42  3.46  3.38  17171744.0    -0.008734\n",
      "2020-07-06         68  3.42   3.64  3.76  3.42  48155956.0     0.062343\n",
      "2020-07-07         68  3.71   3.61  3.79  3.53  42723215.0    -0.008276\n",
      "            StockCode   Open  Close   High    Low     Volume  Log_Returns\n",
      "Date                                                                     \n",
      "2023-12-25     873122  27.60  28.44  28.99  27.60  2348147.0     0.015592\n",
      "2023-12-26     873122  28.57  31.73  31.80  28.02  5269096.0     0.109466\n",
      "2023-12-27     873122  31.00  30.03  31.47  29.20  3044626.0    -0.055066\n",
      "2023-12-28     873122  30.29  29.18  31.58  29.18  2689421.0    -0.028713\n",
      "2023-12-29     873122  28.40  29.21  29.80  28.20  2472944.0     0.001028\n"
     ]
    }
   ],
   "source": [
    "df['Log_Returns'] = np.log(df.Close) - np.log(df.Close.shift(1))\n",
    "\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Trading Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            StockCode  Open  Close  High   Low      Volume  Log_Returns  \\\n",
      "Date                                                                      \n",
      "2020-07-01         68  3.52   3.44  3.52  3.37  25483461.0          NaN   \n",
      "2020-07-02         68  3.40   3.45  3.45  3.38  17180117.0     0.002903   \n",
      "2020-07-03         68  3.45   3.42  3.46  3.38  17171744.0    -0.008734   \n",
      "2020-07-06         68  3.42   3.64  3.76  3.42  48155956.0     0.062343   \n",
      "2020-07-07         68  3.71   3.61  3.79  3.53  42723215.0    -0.008276   \n",
      "\n",
      "            Log_Trading_Range  \n",
      "Date                           \n",
      "2020-07-01           0.043548  \n",
      "2020-07-02           0.020499  \n",
      "2020-07-03           0.023393  \n",
      "2020-07-06           0.094778  \n",
      "2020-07-07           0.071068  \n"
     ]
    }
   ],
   "source": [
    "df['Log_Trading_Range'] = np.log(df.High) - np.log(df.Low)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Volume Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            StockCode  Open  Close  High   Low      Volume  Log_Returns  \\\n",
      "Date                                                                      \n",
      "2020-07-01         68  3.52   3.44  3.52  3.37  25483461.0          NaN   \n",
      "2020-07-02         68  3.40   3.45  3.45  3.38  17180117.0     0.002903   \n",
      "2020-07-03         68  3.45   3.42  3.46  3.38  17171744.0    -0.008734   \n",
      "2020-07-06         68  3.42   3.64  3.76  3.42  48155956.0     0.062343   \n",
      "2020-07-07         68  3.71   3.61  3.79  3.53  42723215.0    -0.008276   \n",
      "\n",
      "            Log_Trading_Range  Log_Volume_Change  \n",
      "Date                                              \n",
      "2020-07-01           0.043548                NaN  \n",
      "2020-07-02           0.020499          -0.394277  \n",
      "2020-07-03           0.023393          -0.000487  \n",
      "2020-07-06           0.094778           1.031180  \n",
      "2020-07-07           0.071068          -0.119702  \n"
     ]
    }
   ],
   "source": [
    "df['Log_Volume_Change'] = np.log(df.Volume) - np.log(df.Volume.shift(1))\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous 10-day Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            StockCode   Open  Close   High    Low     Volume  Log_Returns  \\\n",
      "Date                                                                        \n",
      "2023-12-25     873122  27.60  28.44  28.99  27.60  2348147.0     0.015592   \n",
      "2023-12-26     873122  28.57  31.73  31.80  28.02  5269096.0     0.109466   \n",
      "2023-12-27     873122  31.00  30.03  31.47  29.20  3044626.0    -0.055066   \n",
      "2023-12-28     873122  30.29  29.18  31.58  29.18  2689421.0    -0.028713   \n",
      "2023-12-29     873122  28.40  29.21  29.80  28.20  2472944.0     0.001028   \n",
      "\n",
      "            Log_Trading_Range  Log_Volume_Change  Previous_10_Day_Volatility  \n",
      "Date                                                                          \n",
      "2023-12-25           0.049135          -0.163689                    0.095524  \n",
      "2023-12-26           0.126548           0.808232                    0.103461  \n",
      "2023-12-27           0.074866          -0.548481                    0.091971  \n",
      "2023-12-28           0.079040          -0.124052                    0.087452  \n",
      "2023-12-29           0.055186          -0.083917                    0.087061  \n"
     ]
    }
   ],
   "source": [
    "df['Previous_10_Day_Volatility'] = df['Log_Returns'].rolling(window = 10).std()\n",
    "\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous 30-day Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            StockCode  Open  Close  High   Low      Volume  Log_Returns  \\\n",
      "Date                                                                      \n",
      "2020-07-01         68  3.52   3.44  3.52  3.37  25483461.0          NaN   \n",
      "2020-07-02         68  3.40   3.45  3.45  3.38  17180117.0     0.002903   \n",
      "2020-07-03         68  3.45   3.42  3.46  3.38  17171744.0    -0.008734   \n",
      "2020-07-06         68  3.42   3.64  3.76  3.42  48155956.0     0.062343   \n",
      "2020-07-07         68  3.71   3.61  3.79  3.53  42723215.0    -0.008276   \n",
      "\n",
      "            Log_Trading_Range  Log_Volume_Change  Previous_10_Day_Volatility  \\\n",
      "Date                                                                           \n",
      "2020-07-01           0.043548                NaN                         NaN   \n",
      "2020-07-02           0.020499          -0.394277                         NaN   \n",
      "2020-07-03           0.023393          -0.000487                         NaN   \n",
      "2020-07-06           0.094778           1.031180                         NaN   \n",
      "2020-07-07           0.071068          -0.119702                         NaN   \n",
      "\n",
      "            Previous_30_Day_Volatility  \n",
      "Date                                    \n",
      "2020-07-01                         NaN  \n",
      "2020-07-02                         NaN  \n",
      "2020-07-03                         NaN  \n",
      "2020-07-06                         NaN  \n",
      "2020-07-07                         NaN  \n"
     ]
    }
   ],
   "source": [
    "df['Previous_30_Day_Volatility'] = df['Log_Returns'].rolling(window = 30).std()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next 10-days volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            StockCode  Open  Close  High   Low      Volume  Log_Returns  \\\n",
      "Date                                                                      \n",
      "2020-07-01         68  3.52   3.44  3.52  3.37  25483461.0          NaN   \n",
      "2020-07-02         68  3.40   3.45  3.45  3.38  17180117.0     0.002903   \n",
      "2020-07-03         68  3.45   3.42  3.46  3.38  17171744.0    -0.008734   \n",
      "2020-07-06         68  3.42   3.64  3.76  3.42  48155956.0     0.062343   \n",
      "2020-07-07         68  3.71   3.61  3.79  3.53  42723215.0    -0.008276   \n",
      "\n",
      "            Log_Trading_Range  Log_Volume_Change  Previous_10_Day_Volatility  \\\n",
      "Date                                                                           \n",
      "2020-07-01           0.043548                NaN                         NaN   \n",
      "2020-07-02           0.020499          -0.394277                         NaN   \n",
      "2020-07-03           0.023393          -0.000487                         NaN   \n",
      "2020-07-06           0.094778           1.031180                         NaN   \n",
      "2020-07-07           0.071068          -0.119702                         NaN   \n",
      "\n",
      "            Previous_30_Day_Volatility  Next_10_Days_Volatility  \n",
      "Date                                                             \n",
      "2020-07-01                         NaN                      NaN  \n",
      "2020-07-02                         NaN                 0.060852  \n",
      "2020-07-03                         NaN                 0.061247  \n",
      "2020-07-06                         NaN                 0.060991  \n",
      "2020-07-07                         NaN                 0.058873  \n"
     ]
    }
   ],
   "source": [
    "df['Next_10_Days_Volatility'] = df['Log_Returns'].iloc[::-1].rolling(window = 10).std().iloc[::-1]\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            StockCode  Open  Close  High   Low      Volume  Log_Returns  \\\n",
      "Date                                                                      \n",
      "2020-08-12         68  3.66   3.70  3.77  3.62  11132835.0     0.000000   \n",
      "2020-08-13         68  3.69   3.69  3.76  3.67   9055000.0    -0.002706   \n",
      "2020-08-14         68  3.69   3.69  3.72  3.62  10875834.0     0.000000   \n",
      "2020-08-17         68  3.70   3.76  3.79  3.68  11417667.0     0.018792   \n",
      "2020-08-18         68  3.76   3.89  3.96  3.72  33591392.0     0.033990   \n",
      "\n",
      "            Log_Trading_Range  Log_Volume_Change  Previous_10_Day_Volatility  \\\n",
      "Date                                                                           \n",
      "2020-08-12           0.040601          -0.112175                    0.025826   \n",
      "2020-08-13           0.024227          -0.206582                    0.025869   \n",
      "2020-08-14           0.027250           0.183226                    0.025169   \n",
      "2020-08-17           0.029453           0.048619                    0.015730   \n",
      "2020-08-18           0.062520           1.079108                    0.019106   \n",
      "\n",
      "            Previous_30_Day_Volatility  Next_10_Days_Volatility  \n",
      "Date                                                             \n",
      "2020-08-12                    0.039226                 0.024578  \n",
      "2020-08-13                    0.039237                 0.026290  \n",
      "2020-08-14                    0.039185                 0.026389  \n",
      "2020-08-17                    0.037670                 0.026389  \n",
      "2020-08-18                    0.038096                 0.025103  \n"
     ]
    }
   ],
   "source": [
    "df.dropna(inplace = True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GARCH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Garch predictions for the entire dataset of SPX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a new dataframe for splitting the dataframe in test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Cannot get left slice bound for non-unique label: Timestamp('2020-07-01 00:00:00')\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m df[df\u001b[38;5;241m.\u001b[39mfirst_valid_index():df\u001b[38;5;241m.\u001b[39mlast_valid_index()\u001b[38;5;241m-\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtimedelta(\u001b[38;5;241m1500\u001b[39m)]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m (X\u001b[38;5;241m.\u001b[39mtail())\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pandas\\core\\frame.py:4085\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4083\u001b[0m \u001b[38;5;66;03m# Do we have a slicer (on rows)?\u001b[39;00m\n\u001b[0;32m   4084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m-> 4085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_slice(key)\n\u001b[0;32m   4087\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) DataFrame?\u001b[39;00m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, DataFrame):\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pandas\\core\\generic.py:4349\u001b[0m, in \u001b[0;36mNDFrame._getitem_slice\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4345\u001b[0m \u001b[38;5;124;03m__getitem__ for the case where the key is a slice object.\u001b[39;00m\n\u001b[0;32m   4346\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4347\u001b[0m \u001b[38;5;66;03m# _convert_slice_indexer to determine if this slice is positional\u001b[39;00m\n\u001b[0;32m   4348\u001b[0m \u001b[38;5;66;03m#  or label based, and if the latter, convert to positional\u001b[39;00m\n\u001b[1;32m-> 4349\u001b[0m slobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_convert_slice_indexer(key, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetitem\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(slobj, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m   4351\u001b[0m     \u001b[38;5;66;03m# reachable with DatetimeIndex\u001b[39;00m\n\u001b[0;32m   4352\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_indices_to_slice(\n\u001b[0;32m   4353\u001b[0m         slobj\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mintp, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   4354\u001b[0m     )\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:4281\u001b[0m, in \u001b[0;36mIndex._convert_slice_indexer\u001b[1;34m(self, key, kind)\u001b[0m\n\u001b[0;32m   4279\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m key\n\u001b[0;32m   4280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4281\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice_indexer(start, stop, step)\n\u001b[0;32m   4283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexer\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\datetimes.py:682\u001b[0m, in \u001b[0;36mDatetimeIndex.slice_indexer\u001b[1;34m(self, start, end, step)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;66;03m# GH#33146 if start and end are combinations of str and None and Index is not\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;66;03m# monotonic, we can not use Index.slice_indexer because it does not honor the\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;66;03m# actual elements, is only searching for start and end\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    678\u001b[0m     check_str_or_none(start)\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m check_str_or_none(end)\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_monotonic_increasing\n\u001b[0;32m    681\u001b[0m ):\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Index\u001b[38;5;241m.\u001b[39mslice_indexer(\u001b[38;5;28mself\u001b[39m, start, end, step)\n\u001b[0;32m    684\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    685\u001b[0m in_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6662\u001b[0m, in \u001b[0;36mIndex.slice_indexer\u001b[1;34m(self, start, end, step)\u001b[0m\n\u001b[0;32m   6618\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mslice_indexer\u001b[39m(\n\u001b[0;32m   6619\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   6620\u001b[0m     start: Hashable \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   6621\u001b[0m     end: Hashable \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   6622\u001b[0m     step: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   6623\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mslice\u001b[39m:\n\u001b[0;32m   6624\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6625\u001b[0m \u001b[38;5;124;03m    Compute the slice indexer for input labels and step.\u001b[39;00m\n\u001b[0;32m   6626\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6660\u001b[0m \u001b[38;5;124;03m    slice(1, 3, None)\u001b[39;00m\n\u001b[0;32m   6661\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6662\u001b[0m     start_slice, end_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice_locs(start, end, step\u001b[38;5;241m=\u001b[39mstep)\n\u001b[0;32m   6664\u001b[0m     \u001b[38;5;66;03m# return a slice\u001b[39;00m\n\u001b[0;32m   6665\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(start_slice):\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6879\u001b[0m, in \u001b[0;36mIndex.slice_locs\u001b[1;34m(self, start, end, step)\u001b[0m\n\u001b[0;32m   6877\u001b[0m start_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   6878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 6879\u001b[0m     start_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_slice_bound(start, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_slice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   6881\u001b[0m     start_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mD:\\anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6812\u001b[0m, in \u001b[0;36mIndex.get_slice_bound\u001b[1;34m(self, label, side)\u001b[0m\n\u001b[0;32m   6810\u001b[0m     slc \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_booleans_to_slice(slc\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu1\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   6811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(slc, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m-> 6812\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m   6813\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mside\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m slice bound for non-unique \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6814\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(original_label)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   6815\u001b[0m         )\n\u001b[0;32m   6817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(slc, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m   6818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m side \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Cannot get left slice bound for non-unique label: Timestamp('2020-07-01 00:00:00')\""
     ]
    }
   ],
   "source": [
    "\n",
    "X = df[df.first_valid_index():df.last_valid_index()- datetime.timedelta(1500)]\n",
    "\n",
    "print (X.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a GARCH model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GARCH_model = arch_model(X['Log_Returns'], vol='Garch', p=1, q=1, rescale=False)\n",
    "\n",
    "x = GARCH_model.fit(disp='off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making rolling predictions using the GARCH Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = GARCH_model.fit(disp='off')\n",
    "GARCH_rolling_predictions = pd.DataFrame(\n",
    "    model_fit.forecast(horizon=len(X) - 50).variance,\n",
    "    index=X.index[50:],\n",
    "    columns=['GARCH_rolling_predictions']\n",
    ")\n",
    "\n",
    "print(GARCH_rolling_predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making forward-looking predictions using the GARCH Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GARCH_forward_looking_predictions = GARCH_model.predict(h=1500)\n",
    "\n",
    "print(GARCH_forward_looking_predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming one of the columns of the GARCH Model Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GARCH_rolling_predictions.rename(columns={'Series':'GARCH_rolling_predictions'}, inplace =True)\n",
    "\n",
    "GARCH_forward_looking_predictions.rename(columns={'Log_Returns':'GARCH_forward_looking_predictions'}, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the new feature to the current dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, GARCH_rolling_predictions], axis=1)\n",
    "df = pd.concat([df, GARCH_forward_looking_predictions], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Nan values with 0s for the GARCH Predictions columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rational for this from Keras's creator:\n",
    "\n",
    "https://stackoverflow.com/questions/52570199/multivariate-lstm-with-missing-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GARCH_forward_looking_predictions'] =  df['GARCH_forward_looking_predictions'].fillna(0)\n",
    "df['GARCH_rolling_predictions'] =  df['GARCH_rolling_predictions'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the results of our transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better to predict VIX prices than realized volatility of SPX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use it to predict VIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at relationship of (5, 10, 30) realized volatility of SPX versus VIX prices (Plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate in Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a new dataframe for splitting the dataframe in test and training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using dropna on several columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_columns_to_dropna(df, column_list):\n",
    "    \n",
    "    for column in column_list:\n",
    "        \n",
    "        df = df[df[column].notna()]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = ['Open', 'Log_Returns','Previous_10_Day_Volatility','Next_10_Days_Volatility','Previous_30_Day_Volatility']\n",
    "\n",
    "df = list_columns_to_dropna(df, column_list)\n",
    "\n",
    "print (df.head())\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the final dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting out the S&P 500 Prices from 1990 to 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close'].plot(label = 'S&P 500', figsize =(16,8), title = 'S&P 500 Stock Prices from 1990 to 2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting out the 10-days forward looking volatility of  S&P 500 Prices from 1990 to 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Next_10_Days_Volatility'].plot(label = 'S&P 500', figsize =(16,8), title = '10-days forward looking volatility of  S&P 500 Prices from 1990 to 2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pearson_correlation_matrix_of_dataframe(size_x,size_y,dataframe,correlation_target,correlation_minimum_criteria):\n",
    "\n",
    "    # Using Pearson Correlation\n",
    "\n",
    "    plt.figure(figsize=(size_x,size_y))\n",
    "    cor = dataframe.corr()\n",
    "    sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "    plt.savefig('Images/pearson_correlation_matrix.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation with output variable\n",
    "\n",
    "    target = abs(cor[correlation_target])\n",
    "\n",
    "    #Selecting and printing highly correlated features\n",
    "\n",
    "    relevant_features = target[target>correlation_minimum_criteria]\n",
    "    print(relevant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_pearson_correlation_matrix_of_dataframe(20,20,df,\"Next_10_Days_Volatility\",0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop([\"Next_10_Days_Volatility\",'Low','High','Close','Open','Volume','MACD_h','MACD_sl','RSI14','SMA14','EMA14'], axis=1).values)\n",
    "y = np.array(df[\"Next_10_Days_Volatility\"].values).reshape(-1, 1) \n",
    "\n",
    "test_size = 1500\n",
    "\n",
    "X_train = X[test_size:,]\n",
    "X_test = X[:test_size,]\n",
    "y_train = y[test_size:]\n",
    "y_test = y[:test_size]\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function to get lagged versions of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function increases the number of features of the dataset by \"lagging\" every feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lagged(x, y, t, s):\n",
    "    \n",
    "    lagged = []\n",
    "    \n",
    "    for i in range(x.shape[0] - t):\n",
    "        \n",
    "        if i == x.shape[0] - t:\n",
    "            \n",
    "            break\n",
    "            \n",
    "        for k in range(t):\n",
    "            \n",
    "            if k < t:\n",
    "                \n",
    "                lagged.append(x[i+k])\n",
    "                \n",
    "    lagged = np.array(lagged).reshape(s)\n",
    "    \n",
    "    return lagged, y[:lagged.shape[0],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "\n",
    "X_train, y_train = get_lagged(X_train, y_train, N, (X_train.shape[0]-N, N*X_train.shape[1]))\n",
    "X_test, y_test = get_lagged(X_test, y_test, N, (X_test.shape[0]-N, N*X_test.shape[1]))\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 4\n",
    "\n",
    "X_train, y_train = get_lagged(X_train, y_train, T, (X_train.shape[0]-T, T, X_train.shape[1]))\n",
    "X_test, y_test = get_lagged(X_test, y_test, T, (X_test.shape[0]-T, T, X_test.shape[1]))\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLSTM = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "y = LSTM(200, return_sequences=True)(inputLSTM)\n",
    "y = LSTM(200)(y)\n",
    "y = Dense(1)(y)\n",
    "lstm = Model(inputs=inputLSTM, outputs=y)\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting out the LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(lstm, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring the parameters of the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.compile(optimizer=keras.optimizers.Adam(lr=0.01),loss=tf.keras.losses.MeanSquaredError(),metrics=[tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data for SPX where you have Open Close and Volumen (1960)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = lstm.fit(X_train, y_train,batch_size=700,epochs=60,verbose=1,validation_split=0.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the RSME for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['root_mean_squared_error'])\n",
    "plt.plot(hist.history['val_root_mean_squared_error'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the predictions made by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, i in enumerate(lstm.predict(X_test)):\n",
    "    \n",
    "    print('Prediction: ' + str('{:.2f}'.format(round(100 * round(i[0], 4),3))) + ',    ' + 'Actual Value: ' + str('{:.2f}'.format(round(100 * round(y_test[ind][0],4),2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the results of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printing_out_results_of_a_model(model,X_test,y_test):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Print the R2 score \n",
    "\n",
    "    print (\"R2 score:\\n\") \n",
    "    print (('{:.2f}'.format((100*(r2_score(y_test, y_pred))))) + \" %\")\n",
    "\n",
    "    print (\"\\n\")\n",
    "    \n",
    "    # Print the RMSE\n",
    "\n",
    "    print (\"RMSE:\\n\")\n",
    "    print (math.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    \n",
    "    print ('\\n')\n",
    "    \n",
    "    # Print the mean squared error\n",
    "    \n",
    "    print (\"Mean Squared Error:\\n\")\n",
    "    print (mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printing_out_results_of_a_model(lstm, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
