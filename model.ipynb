{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Paper on LSTM Neural Networks for stock prices volatility prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://cs230.stanford.edu/projects_fall_2019/reports/26254244.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial for building an LSTM neural network for time-series prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from pandas_datareader import data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Datetime\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Scikit-Learn\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# GARCH model\n",
    "\n",
    "import pyflux as pf\n",
    "\n",
    "# Keras\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "# Tensorflow\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the csv file with the financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'input/financial_data.csv')\n",
    "\n",
    "print (df.head())\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see here, we have 254 columns, corresponding to the 254 business days for which we have financial data and 10 columns, which are the 10 financial indicators we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposing the dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are working with time series data, we should have the dates as one column and will thus use transpose() for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.transpose()\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset the index of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming the columns with the financial indicators name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    \n",
    "    df.columns[0]: 'Date',\n",
    "    df.columns[1]:'Open',\n",
    "    df.columns[2]: 'Close',\n",
    "    df.columns[3]:'High',\n",
    "    df.columns[4]:'Low',\n",
    "    df.columns[5]: 'Volume',\n",
    "    df.columns[6]: 'RSI14',\n",
    "    df.columns[7]:'SMA14',\n",
    "    df.columns[8]: 'EMA14',\n",
    "    df.columns[9]:'MACD_sl',\n",
    "    df.columns[10]:'MACD_h'\n",
    "\n",
    "})\n",
    "\n",
    "print (df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Date column into a Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] =  pd.to_datetime(df['Date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Date column as the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding volume from Yahoo Finance API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '1990-06-11'\n",
    "end_date = '2020-06-10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_data = data.DataReader('^GSPC', 'yahoo', start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (panel_data.head())\n",
    "\n",
    "print( panel_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Volume'] =  panel_data['Volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to print out the data type of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_columns_to_dropna(df, column_list):\n",
    "    \n",
    "    for column in column_list:\n",
    "        \n",
    "        df = df[df[column].notna()]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = ['Open','Close']\n",
    "\n",
    "df = list_columns_to_dropna(df, column_list)\n",
    "\n",
    "print (df.head())\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_type_of_dataframe_columns(df):\n",
    "    \n",
    "    dataTypeSeries = df.dtypes\n",
    " \n",
    "    print('Data type of each column of Dataframe :')\n",
    "    print(dataTypeSeries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithmic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Log_Returns'] = np.log(df.Close) - np.log(df.Close.shift(1))\n",
    "\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Trading Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Log_Trading_Range'] = np.log(df.High) - np.log(df.Low)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Volume Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Log_Volume_Change'] = np.log(df.Volume) - np.log(df.Volume.shift(1))\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous 10-day Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Previous_10_Day_Volatility'] = df['Log_Returns'].rolling(window = 10).std()\n",
    "\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous 30-day Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Previous_30_Day_Volatility'] = df['Log_Returns'].rolling(window = 30).std()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next 10-days volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Next_10_Days_Volatility'] = df['Log_Returns'].iloc[::-1].rolling(window = 10).std().iloc[::-1]\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'output/output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GARCH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Garch predictions for the entire dataset of SPX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a new dataframe for splitting the dataframe in test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.first_valid_index():df.last_valid_index()- datetime.timedelta(1500)]\n",
    "\n",
    "print (X.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a GARCH model using PyFlux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GARCH_model = pf.GARCH(X, target = 'Log_Returns', p=1, q=1)\n",
    "\n",
    "x = GARCH_model.fit()\n",
    "\n",
    "x.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making rolling predictions using the GARCH Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GARCH_rolling_predictions = GARCH_model.predict_is(h = len(X) - 50, fit_once = True)\n",
    "\n",
    "print(GARCH_rolling_predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making forward-looking predictions using the GARCH Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GARCH_forward_looking_predictions = GARCH_model.predict(h=1500)\n",
    "\n",
    "print(GARCH_forward_looking_predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming one of the columns of the GARCH Model Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GARCH_rolling_predictions.rename(columns={'Series':'GARCH_rolling_predictions'}, inplace =True)\n",
    "\n",
    "GARCH_forward_looking_predictions.rename(columns={'Log_Returns':'GARCH_forward_looking_predictions'}, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the new feature to the current dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, GARCH_rolling_predictions], axis=1)\n",
    "df = pd.concat([df, GARCH_forward_looking_predictions], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Nan values with 0s for the GARCH Predictions columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rational for this from Keras's creator:\n",
    "\n",
    "https://stackoverflow.com/questions/52570199/multivariate-lstm-with-missing-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GARCH_forward_looking_predictions'] =  df['GARCH_forward_looking_predictions'].fillna(0)\n",
    "df['GARCH_rolling_predictions'] =  df['GARCH_rolling_predictions'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the results of our transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better to predict VIX prices than realized volatility of SPX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use it to predict VIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at relationship of (5, 10, 30) realized volatility of SPX versus VIX prices (Plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate in Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a new dataframe for splitting the dataframe in test and training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using dropna on several columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_columns_to_dropna(df, column_list):\n",
    "    \n",
    "    for column in column_list:\n",
    "        \n",
    "        df = df[df[column].notna()]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = ['Open', 'Log_Returns','Previous_10_Day_Volatility','Next_10_Days_Volatility','Previous_30_Day_Volatility']\n",
    "\n",
    "df = list_columns_to_dropna(df, column_list)\n",
    "\n",
    "print (df.head())\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the final dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'output/output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting out the S&P 500 Prices from 1990 to 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close'].plot(label = 'S&P 500', figsize =(16,8), title = 'S&P 500 Stock Prices from 1990 to 2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting out the 10-days forward looking volatility of  S&P 500 Prices from 1990 to 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Next_10_Days_Volatility'].plot(label = 'S&P 500', figsize =(16,8), title = '10-days forward looking volatility of  S&P 500 Prices from 1990 to 2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pearson_correlation_matrix_of_dataframe(size_x,size_y,dataframe,correlation_target,correlation_minimum_criteria):\n",
    "\n",
    "    # Using Pearson Correlation\n",
    "\n",
    "    plt.figure(figsize=(size_x,size_y))\n",
    "    cor = dataframe.corr()\n",
    "    sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "    plt.savefig('Images/pearson_correlation_matrix.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Correlation with output variable\n",
    "\n",
    "    target = abs(cor[correlation_target])\n",
    "\n",
    "    #Selecting and printing highly correlated features\n",
    "\n",
    "    relevant_features = target[target>correlation_minimum_criteria]\n",
    "    print(relevant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_pearson_correlation_matrix_of_dataframe(20,20,df,\"Next_10_Days_Volatility\",0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop([\"Next_10_Days_Volatility\",'Low','High','Close','Open','Volume','MACD_h','MACD_sl','RSI14','SMA14','EMA14'], axis=1).values)\n",
    "y = np.array(df[\"Next_10_Days_Volatility\"].values).reshape(-1, 1) \n",
    "\n",
    "test_size = 1500\n",
    "\n",
    "X_train = X[test_size:,]\n",
    "X_test = X[:test_size,]\n",
    "y_train = y[test_size:]\n",
    "y_test = y[:test_size]\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function to get lagged versions of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function increases the number of features of the dataset by \"lagging\" every feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lagged(x, y, t, s):\n",
    "    \n",
    "    lagged = []\n",
    "    \n",
    "    for i in range(x.shape[0] - t):\n",
    "        \n",
    "        if i == x.shape[0] - t:\n",
    "            \n",
    "            break\n",
    "            \n",
    "        for k in range(t):\n",
    "            \n",
    "            if k < t:\n",
    "                \n",
    "                lagged.append(x[i+k])\n",
    "                \n",
    "    lagged = np.array(lagged).reshape(s)\n",
    "    \n",
    "    return lagged, y[:lagged.shape[0],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "\n",
    "X_train, y_train = get_lagged(X_train, y_train, N, (X_train.shape[0]-N, N*X_train.shape[1]))\n",
    "X_test, y_test = get_lagged(X_test, y_test, N, (X_test.shape[0]-N, N*X_test.shape[1]))\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 4\n",
    "\n",
    "X_train, y_train = get_lagged(X_train, y_train, T, (X_train.shape[0]-T, T, X_train.shape[1]))\n",
    "X_test, y_test = get_lagged(X_test, y_test, T, (X_test.shape[0]-T, T, X_test.shape[1]))\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLSTM = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "y = LSTM(200, return_sequences=True)(inputLSTM)\n",
    "y = LSTM(200)(y)\n",
    "y = Dense(1)(y)\n",
    "lstm = Model(inputs=inputLSTM, outputs=y)\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting out the LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(lstm, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring the parameters of the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.compile(optimizer=keras.optimizers.Adam(lr=0.01),loss=tf.keras.losses.MeanSquaredError(),metrics=[tf.keras.metrics.RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data for SPX where you have Open Close and Volumen (1960)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 85ms/step - loss: 3.8515e-06 - root_mean_squared_error: 0.0020 - val_loss: 4.7232e-06 - val_root_mean_squared_error: 0.0022\n",
      "Epoch 48/60\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 3.7894e-06 - root_mean_squared_error: 0.0019 - val_loss: 4.6308e-06 - val_root_mean_squared_error: 0.0022\n",
      "Epoch 49/60\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 3.7271e-06 - root_mean_squared_error: 0.0019 - val_loss: 4.5384e-06 - val_root_mean_squared_error: 0.0021\n",
      "Epoch 50/60\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 3.6649e-06 - root_mean_squared_error: 0.0019 - val_loss: 4.4461e-06 - val_root_mean_squared_error: 0.0021\n",
      "Epoch 51/60\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 3.6026e-06 - root_mean_squared_error: 0.0019 - val_loss: 4.3541e-06 - val_root_mean_squared_error: 0.0021\n",
      "Epoch 52/60\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 3.5403e-06 - root_mean_squared_error: 0.0019 - val_loss: 4.2622e-06 - val_root_mean_squared_error: 0.0021\n",
      "Epoch 53/60\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 3.4781e-06 - root_mean_squared_error: 0.0019 - val_loss: 4.1706e-06 - val_root_mean_squared_error: 0.0020\n",
      "Epoch 54/60\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 3.4161e-06 - root_mean_squared_error: 0.0018 - val_loss: 4.0793e-06 - val_root_mean_squared_error: 0.0020\n",
      "Epoch 55/60\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 3.3543e-06 - root_mean_squared_error: 0.0018 - val_loss: 3.9883e-06 - val_root_mean_squared_error: 0.0020\n",
      "Epoch 56/60\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 3.2927e-06 - root_mean_squared_error: 0.0018 - val_loss: 3.8977e-06 - val_root_mean_squared_error: 0.0020\n",
      "Epoch 57/60\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 3.2315e-06 - root_mean_squared_error: 0.0018 - val_loss: 3.8075e-06 - val_root_mean_squared_error: 0.0020\n",
      "Epoch 58/60\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 3.1707e-06 - root_mean_squared_error: 0.0018 - val_loss: 3.7179e-06 - val_root_mean_squared_error: 0.0019\n",
      "Epoch 59/60\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 3.1104e-06 - root_mean_squared_error: 0.0018 - val_loss: 3.6288e-06 - val_root_mean_squared_error: 0.0019\n",
      "Epoch 60/60\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 3.0506e-06 - root_mean_squared_error: 0.0017 - val_loss: 3.5404e-06 - val_root_mean_squared_error: 0.0019\n"
     ]
    }
   ],
   "source": [
    "hist = lstm.fit(X_train, y_train,batch_size=700,epochs=60,verbose=1,validation_split=0.3,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the RSME for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['root_mean_squared_error'])\n",
    "plt.plot(hist.history['val_root_mean_squared_error'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the predictions made by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, i in enumerate(lstm.predict(X_test)):\n",
    "    \n",
    "    print('Prediction: ' + str('{:.2f}'.format(round(100 * round(i[0], 4),3))) + ',    ' + 'Actual Value: ' + str('{:.2f}'.format(round(100 * round(y_test[ind][0],4),2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing out the results of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printing_out_results_of_a_model(model,X_test,y_test):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Print the R2 score \n",
    "\n",
    "    print (\"R2 score:\\n\") \n",
    "    print (('{:.2f}'.format((100*(r2_score(y_test, y_pred))))) + \" %\")\n",
    "\n",
    "    print (\"\\n\")\n",
    "    \n",
    "    # Print the RMSE\n",
    "\n",
    "    print (\"RMSE:\\n\")\n",
    "    print (math.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    \n",
    "    print ('\\n')\n",
    "    \n",
    "    # Print the mean squared error\n",
    "    \n",
    "    print (\"Mean Squared Error:\\n\")\n",
    "    print (mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printing_out_results_of_a_model(lstm, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
